#!/usr/bin/env python3
"""
arxiv_notifier.py
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
* AI ìš”ì•½ í† ê¸€: AI_SUMMARIZE = True / False
* DAYS_BACK: ìµœê·¼ N ì¼ ë…¼ë¬¸ë§Œ ë©”ì¼ì— í¬í•¨
* GPT ëª¨ë¸â€†Â·â€†API í‚¤ëŠ” í™˜ê²½ë³€ìˆ˜(OPENAI_API_KEY)ë¡œ ê´€ë¦¬
"""

from __future__ import annotations

import email.utils as eut
import hashlib
import json
import os
import smtplib
import urllib.parse
from datetime import date, datetime, timedelta, timezone
from email.header import Header
from email.mime.text import MIMEText
from typing import Any, Dict, List, Optional

import feedparser

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ AI ìš”ì•½ ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
AI_SUMMARIZE = True                # â† True/False
MODEL_ID = "gpt-4.5-preview"       # í•„ìš” ì‹œ ë³€ê²½
if AI_SUMMARIZE:
    import openai

    openai.api_key = os.getenv("OPENAI_API_KEY")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ê¸°ë³¸ ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ENV_VARS = ("EMAIL_ADDRESS", "EMAIL_PASSWORD", "TO_EMAIL")
TOPIC_FILE = "topics.json"
MAX_RESULTS_DEFAULT = 10
DAYS_BACK = 1                      # ìµœê·¼ N ì¼ ë…¼ë¬¸
TITLE_MAX, ABSTRACT_MAX = 120, 600
GLOBAL_EXCLUDE = {"review", "survey", "comment on", "corrigendum"}

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ìœ í‹¸ í•¨ìˆ˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def getenv_or_exit(name: str) -> str:
    val = os.getenv(name)
    if not val:
        raise SystemExit(f"[config] '{name}' í™˜ê²½ë³€ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤")
    return val


def load_topics(path: str) -> Dict[str, Any]:
    with open(path, encoding="utf-8") as fh:
        return json.load(fh)


def make_query(keyword: str, cats: List[str]) -> str:
    phrase = urllib.parse.quote_plus(f'"{keyword}"')
    kw = f"(ti:{phrase}+OR+abs:{phrase})"
    if cats:
        cat = "+OR+".join(f"cat:{c}" for c in cats)
        return f"({cat})+AND+{kw}"
    return kw


def fetch_entries(query: str, n: int) -> List[Any]:
    url = (
        "http://export.arxiv.org/api/query?search_query="
        f"{query}&start=0&max_results={n}"
        "&sortBy=submittedDate&sortOrder=descending"
    )
    return feedparser.parse(url).entries


def _get_entry_datetime(entry) -> Optional[datetime]:
    """Return the first parseable date field in UTC, else None."""
    for field in ("published", "updated", "created"):
        val = getattr(entry, field, None)
        if not val:
            continue
        tup = eut.parsedate_tz(val)
        if tup:
            return datetime.fromtimestamp(eut.mktime_tz(tup), tz=timezone.utc)
    return None


def is_recent(entry) -> bool:
    """True â†” entry submitted within DAYS_BACK days; False otherwise."""
    dt = _get_entry_datetime(entry)
    if dt is None:
        return False  # skip if no valid date
    return dt >= datetime.now(tz=timezone.utc) - timedelta(days=DAYS_BACK)


def truncate(text: str, limit: int) -> str:
    return text if len(text) <= limit else text[: limit - 3].rstrip() + "..."


def summarize(title: str, abstract: str) -> str:
    resp = openai.chat.completions.create(
        model=MODEL_ID,
        messages=[
            {
                "role": "system",
                "content": (
                    "You are a scientific summarizer.\n"
                    "Return exactly three lines:\n"
                    "1) Problem: <one concise sentence>\n"
                    "2) Result: <one concise sentence>\n"
                    "3) Method: <one concise sentence>"
                ),
            },
            {"role": "user", "content": f"TITLE: {title}\nTEXT: {abstract}"},
        ],
        temperature=0.3,
        max_tokens=120,
    )
    return resp.choices[0].message.content.strip()


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ë…¼ë¬¸ ìˆ˜ì§‘ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def collect_papers(topics: Dict[str, Any]) -> Dict[str, List[Dict[str, str]]]:
    results, seen = {}, set()

    for topic, cfg in topics.items():
        papers = []
        for kw in cfg.get("keywords", []):
            for entry in fetch_entries(
                make_query(kw, cfg.get("categories", [])),
                int(cfg.get("max_results", MAX_RESULTS_DEFAULT)),
            ):
                if not is_recent(entry):
                    continue  # too old or no date

                uid = hashlib.sha1(entry.id.encode()).hexdigest()
                if uid in seen:
                    continue
                seen.add(uid)

                title = " ".join(entry.title.split())
                abstract = " ".join(entry.summary.split())
                if any(k in abstract.lower() for k in GLOBAL_EXCLUDE):
                    continue

                info = {
                    "title": truncate(title, TITLE_MAX),
                    "link": entry.link,
                    "abstract": truncate(abstract, ABSTRACT_MAX),
                }
                if AI_SUMMARIZE:
                    try:
                        info["summary"] = summarize(title, abstract)
                    except Exception as err:
                        info["summary"] = f"(ìš”ì•½ ì‹¤íŒ¨: {err})"

                papers.append(info)

        if papers:
            results[topic] = papers

    return results


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ë©”ì¼ ë³¸ë¬¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def build_email(papers: Dict[str, List[Dict[str, str]]]) -> str:
    lines = ["ğŸ“°  ì˜¤ëŠ˜ì˜ arXiv\n"]
    for i, (topic, plist) in enumerate(papers.items()):
        lines += [f"ğŸ“Œ {topic.upper()} ({len(plist)})", "=" * (len(topic) + 7)]

        for j, p in enumerate(plist, 1):
            if AI_SUMMARIZE:
                lines.append("   ğŸ’¡ 3-line summary:")
                for line in p["summary"].splitlines():
                    lines.append(f"      {line}")
                lines.append("")

            lines += [
                f"{j}. ğŸ“„ {p['title']}",
                f"   ğŸ”— {p['link']}\n",
                "   ğŸ“ Abstract:",
                f"      {p['abstract']}\n",
            ]
            if j < len(plist):
                lines.append("   " + "-" * 40 + "\n")

        if i < len(papers) - 1:
            lines.append("ãƒ»" * 30 + "\n")

    lines += [
        "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”",
        f"ì§€ë‚œ {DAYS_BACK}ì¼ ì´ë‚´ì— ì œì¶œëœ ë…¼ë¬¸ë§Œ í¬í•¨í–ˆìŠµë‹ˆë‹¤.",
    ]
    return "\n".join(lines)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì´ë©”ì¼ ë°œì†¡ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def send_email(subject: str, body: str, sender: str, pwd: str, recipient: str) -> None:
    msg = MIMEText(body, "plain", "utf-8")
    msg["Subject"], msg["From"], msg["To"] = subject, sender, recipient
    with smtplib.SMTP("smtp.gmail.com", 587) as s:
        s.starttls()
        s.login(sender, pwd)
        s.sendmail(sender, [recipient], msg.as_string())


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ë©”ì¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main() -> None:
    sender, pwd, rcpt = (getenv_or_exit(k) for k in ENV_VARS)
    papers = collect_papers(load_topics(TOPIC_FILE))
    if not papers:
        print("[info] no new papers")
        return

    first_topic, first_list = next(iter(papers.items()))
    subject = str(
        Header(
            f"{date.today():%Y-%m-%d} - ì˜¤ëŠ˜ì˜ arXiv - "
            f"{first_topic} ({len(first_list)})",
            "utf-8",
        )
    )
    send_email(subject, build_email(papers), sender, pwd, rcpt)
    print("[ok] email sent")


if __name__ == "__main__":
    main()
